{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R9qviYfF49EA","trusted":true},"outputs":[],"source":["!pip install transformers -q\n","!pip install request -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-KK15Ig4nxy","trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import tensorflow as tf\n","import torch\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig, Adafactor\n","from transformers import get_linear_schedule_with_warmup\n","import datetime\n","import random\n","import seaborn as sns\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","import requests\n","\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RoHZ44pr9knT","trusted":true},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = 'dbmdz/bert-base-turkish-128k-uncased'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xc9NLV9p4syG","trusted":true},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dHCM_wLw4v2t","trusted":true},"outputs":[],"source":["#df = pd.read_csv('/content/obs_clean_data_not_turkish_char.csv')\n","#df = pd.read_csv('/content/obs_clean_data_turkish_char.csv')\n","#df = pd.read_csv('/content/obs_clean_data.csv')\n","df = pd.read_csv('/kaggle/input/train/teknofest_train_final.csv', sep='|')\n","df['is_offensive'] = df['is_offensive'].astype(int)\n","\n","df.head(10)\n","df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4XyxkqjQUqO","trusted":true},"outputs":[],"source":["# Etiketleri ayrıştırma\n","df = pd.DataFrame(df, columns=['text', 'target'])\n","le = LabelEncoder()\n","df.target = le.fit_transform(df.target)\n","print(df.info())\n","print(df.target.unique())\n","print(df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-NRQEus2wwky","trusted":true},"outputs":[],"source":["url = \"https://cryptic-oasis-68424.herokuapp.com/preprocess?tr_chars=false&acc_marks=true&punct=true&lower=true&offensive=false&norm_numbers=true&remove_numbers=false&remove_spaces=true&remove_stopwords=false&min_len=4\"\n","texts = df.text.values.tolist()\n","preprocess_response = requests.post(url, json={\"texts\": texts})\n","processed_text = preprocess_response.json()['result']\n","df.text = processed_text\n","print(df[df.text == ''].sum())\n","df = df[df['text'] != '']\n","print(df.head())\n","print(df[df.text == ''].sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8aaq7X55bea","trusted":true},"outputs":[],"source":["# check GPU\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","  device = torch.device(\"cuda\")\n","  print('GPU:', torch.cuda.get_device_name(0))\n","else:\n","  raise SystemError('GPU device not found')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egWBTrih5q7U","trusted":true},"outputs":[],"source":["df.groupby('target').size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5WxDtUM6UIe","trusted":true},"outputs":[],"source":["df['text'].size #total data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9b1JSew8KkF","trusted":true},"outputs":[],"source":["\n","df.head(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGfQQi6L6b78","trusted":true},"outputs":[],"source":["df['target'] = LabelEncoder().fit_transform(df['target'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IqAL6yDb8FwS","trusted":true},"outputs":[],"source":["df.head(8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOCi4OFY8O-V","trusted":true},"outputs":[],"source":["\"\"\"\n","INSULT    --> 0\n","OTHER     --> 1\n","PROFANITY --> 2\n","RACIST    --> 3\n","SEXIST    --> 4\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOIi_Zc08os_","trusted":true},"outputs":[],"source":["training = df.groupby('target').apply(lambda x : x.sample(frac = 0.8))\n","test = pd.concat([df,training]).drop_duplicates(keep=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fBq1FXZ86mt","trusted":true},"outputs":[],"source":["training.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSP3TwmZ-IfK","trusted":true},"outputs":[],"source":["test.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I9PpOcEV-UVf","trusted":true},"outputs":[],"source":["print(\"Training: \", len(training))\n","print(\"Test: \", len(test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfuTk969-fnB","trusted":true},"outputs":[],"source":["training_texts = training.text.values\n","training_labels = training.target.values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knwQqHLG-ogr","trusted":true},"outputs":[],"source":["training_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Yna3KtI-q1L","trusted":true},"outputs":[],"source":["training_texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-dEElt4-wDS","trusted":true},"outputs":[],"source":["input_ids = []\n","attention_masks = []\n","max_len = 32\n","\n","\n","for text in training_texts:\n","    encoded_dict = tokenizer.encode_plus(\n","                        str(text),                     \n","                        add_special_tokens = True,\n","                        max_length = max_len,      \n","                        pad_to_max_length = True,\n","                        return_attention_mask = True, \n","                        return_tensors = 'pt',\n","                   )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(training_labels)\n","\n","print('Original: ', training_texts[0])\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lZsMNmGT-wFg","trusted":true},"outputs":[],"source":["train_dataset = TensorDataset(input_ids, attention_masks, labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NqbHBbK7-wHd","trusted":true},"outputs":[],"source":["batch_size = 32\n","\n","train_dataloader = DataLoader(\n","            train_dataset,  \n","            sampler = RandomSampler(train_dataset), \n","            batch_size = batch_size \n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVTJnYur-wJB","trusted":true},"outputs":[],"source":["num_of_cat = len(df['target'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8KFArp6_XLf","trusted":true},"outputs":[],"source":["num_of_cat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_EqcVDE_cjK","trusted":true},"outputs":[],"source":["model = BertForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels = num_of_cat, \n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIevjCf1_clZ","trusted":true},"outputs":[],"source":["epochs = 8 #denemelerim sonucu kayıp 0 a 8. epochta yaklaşıyor\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5,\n","                  eps = 1e-7 \n","                )\n","\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unQWd1Ht_cm1","trusted":true},"outputs":[],"source":["def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4vD38lC_cqe","trusted":true},"outputs":[],"source":["seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","total_t0 = time.time()\n","\n","for epoch_i in range(0, epochs):\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    t0 = time.time()\n","    total_train_loss = 0\n","    model.train()\n","    \n","    for step, batch in enumerate(train_dataloader):\n","        if step % 10 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()        \n","        output = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        loss = output['loss']\n","        logits = output['logits']\n","        total_train_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"Training epoch took: {:}\".format(training_time))\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Training Time': training_time,\n","        }\n","    )\n","\n","print(\"Training completed in {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmqGgmuK_ct3","trusted":true},"outputs":[],"source":["df_stats = pd.DataFrame(data=training_stats)\n","plt.plot(df_stats['Training Loss'], label=\"Training\")\n","plt.title(\"Training Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zg-pi6q1_cvq","trusted":true},"outputs":[],"source":["test_texts = test.text.values\n","test_labels = test.target.values\n","\n","input_ids = []\n","attention_masks = []\n","\n","for text in test_texts:\n","    encoded_dict = tokenizer.encode_plus(\n","                        text,                     \n","                        add_special_tokens = True, \n","                        max_length = max_len,          \n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,  \n","                        return_tensors = 'pt',   \n","                   )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(test_labels)\n","\n","\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gks8uv4g_cxO","trusted":true},"outputs":[],"source":["print('Prediction started on test data')\n","model.eval()\n","predictions , true_labels = [], []\n","\n","for batch in prediction_dataloader:\n","  batch = tuple(t.to(device) for t in batch)\n","  b_input_ids, b_input_mask, b_labels = batch\n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('Prediction completed')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxKBEheh_czC","trusted":true},"outputs":[],"source":["prediction_set = []\n","\n","for i in range(len(true_labels)):\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  prediction_set.append(pred_labels_i)\n","\n","prediction_scores = [item for sublist in prediction_set for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47C8BbWC_c04","trusted":true},"outputs":[],"source":["f_score = f1_score(test_labels, prediction_scores, average='macro')\n","precision = precision_score(test_labels, prediction_scores, average='macro')\n","recall = recall_score(test_labels, prediction_scores, average='macro')\n","accr = accuracy_score(test_labels, prediction_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCnL1adE_c2j","trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","print(\"F-Score Macro: \", f_score)\n","print(\"Recall Macro: \", recall)\n","print(\"Precision Macro: \", precision)\n","print(\"Accuracy: \", accr)\n","class_names =['INSULT','OTHER','PROFANITY','RACIST','SEXIST']\n","cm = confusion_matrix(test_labels, prediction_scores, )\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                              display_labels=class_names)\n","disp.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NN2cdyxx_c4l","trusted":true},"outputs":[],"source":["report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdC3-SdECMnE","trusted":true},"outputs":[],"source":["report = report.rename(columns={\n","                                '0':'INSULT',\n","                                '1':'OTHER',\n","                                '2':'PROFANITY',\n","                                '3':'RACIST',\n","                                '4':'SEXIST'})\n","report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQzT8_owCMwb","trusted":true},"outputs":[],"source":["from transformers import BertTokenizer,BertTokenizerFast, TFBertForSequenceClassification, BertConfig, TFBertModel\n","model_path = \"/content/drive/MyDrive/Nane&Limon/2023 DDI Yarışma dokümantasyonu/bigscience_t0_model\"\n","tokenizer_path = \"/content/drive/MyDrive/Nane&Limon/2023 DDI Yarışma dokümantasyonu/bigscience_t0_tokenizer\"\n","model = TFBertForSequenceClassification.from_pretrained(model_path, from_pt=True) # modify labels as needed.\n","tokenizer = BertTokenizer.from_pretrained(tokenizer_path, do_lower_case=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0MfRkkuRCMzl","trusted":true},"outputs":[],"source":["from transformers import TextClassificationPipeline\n","\n","text = [\"Selam herkese bugün güzel bir gün\",\n","        \"Aptal zihniyetinizde bir Yunan yatıyor\",\n","        \"Akşam halısahaya giderken karısından izin alanda kendine erkeğim demesin!\",\n","        \"kör olası çöpçüler aşkımı süpürmüşler\",\n","        \"sınıfımdaki deve hörgüçleri\",\n","       \"bugün de ölmedik\",\n","       'seninle iyi anlaştık',\n","       'seni sevmek umitli sey ama artik umit yetmiyor bana',\n","       'Selam sen hariç piç',\n","       'bana bak kadın']\n","\n","pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gEGwikzCM1o","trusted":true},"outputs":[],"source":["[print(f\"{text[index]} - {i['label']}\") for index, i in enumerate(pipe(text))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pZ3FcL_m0Ne","trusted":true},"outputs":[],"source":["\"\"\"\n","INSULT    --> 0\n","OTHER     --> 1\n","PROFANITY --> 2\n","RACIST    --> 3\n","SEXIST    --> 4\n","\"\"\""]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
