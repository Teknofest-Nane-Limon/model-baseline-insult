{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0ed04673c64748b0b44d44bf56e9d804":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27286bba47f745d7938122553f983ded","IPY_MODEL_63915f7302fc461e99b7ee6ff6c7e294","IPY_MODEL_2f50e751226a478f8ed5645af1c3062b"],"layout":"IPY_MODEL_b7e9499133b34ff5986b1b4ce8bb3d73"}},"27286bba47f745d7938122553f983ded":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c16eafff2184d1c9ecde97629b3bd6c","placeholder":"​","style":"IPY_MODEL_98d1b0ef1e294f639c035bbd1faa6e14","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"63915f7302fc461e99b7ee6ff6c7e294":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ced53e036b8430eada4d3132d8a6641","max":1233088,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a54216b98e094e78b0166cece05d15cb","value":1233088}},"2f50e751226a478f8ed5645af1c3062b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3d7f6284ec042e9a77aee68002e8692","placeholder":"​","style":"IPY_MODEL_441d5f99cdbb4c9aa7321e6a5e09024f","value":" 1.23M/1.23M [00:00&lt;00:00, 7.07MB/s]"}},"b7e9499133b34ff5986b1b4ce8bb3d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c16eafff2184d1c9ecde97629b3bd6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98d1b0ef1e294f639c035bbd1faa6e14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ced53e036b8430eada4d3132d8a6641":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a54216b98e094e78b0166cece05d15cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3d7f6284ec042e9a77aee68002e8692":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"441d5f99cdbb4c9aa7321e6a5e09024f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dee1d1835fcd42d69f0b343009a5453d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_980d61c0dd5245dc9ce34ba0be168dd7","IPY_MODEL_b482ee544ecc464780f3534f6150ca65","IPY_MODEL_c03dd6d5b99940cf87aae6bd82a07da4"],"layout":"IPY_MODEL_de52ce89909343c6948ad29a4e3793f8"}},"980d61c0dd5245dc9ce34ba0be168dd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17e37e17d8314833ae4c30cd66695acd","placeholder":"​","style":"IPY_MODEL_7169b1d67d8545bebc2c4fc533a7e30a","value":"Downloading (…)okenizer_config.json: 100%"}},"b482ee544ecc464780f3534f6150ca65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8f5f8c3ff854a8f9093d745c489972d","max":59,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6573d3f8f304e2089ef231aef462330","value":59}},"c03dd6d5b99940cf87aae6bd82a07da4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9c350e5dbc14651b9fc4bcb840f5082","placeholder":"​","style":"IPY_MODEL_7ee5c5c006b24a9db6f247af5148f12b","value":" 59.0/59.0 [00:00&lt;00:00, 1.81kB/s]"}},"de52ce89909343c6948ad29a4e3793f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17e37e17d8314833ae4c30cd66695acd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7169b1d67d8545bebc2c4fc533a7e30a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8f5f8c3ff854a8f9093d745c489972d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6573d3f8f304e2089ef231aef462330":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9c350e5dbc14651b9fc4bcb840f5082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ee5c5c006b24a9db6f247af5148f12b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"724209c6fd314416811813dc7b6c360c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff61b71a09974846ae20c71a881f7d79","IPY_MODEL_b193579ca9de44c089e9011b464e754b","IPY_MODEL_fe8e89f3fcfa4caba30248973b371279"],"layout":"IPY_MODEL_b4998147ee8b4f25a0798f95aa4dad38"}},"ff61b71a09974846ae20c71a881f7d79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4b3478080ab4a7894012d1037e8c825","placeholder":"​","style":"IPY_MODEL_6efde522637d444f895a5794a42e8c17","value":"Downloading (…)lve/main/config.json: 100%"}},"b193579ca9de44c089e9011b464e754b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca1f8a7c77b44d0fa5465d103d7b9425","max":386,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d3113a9ff1946168ac7ca3bb266e769","value":386}},"fe8e89f3fcfa4caba30248973b371279":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db4d2fc9ae9e4fb894f97295ec061e5b","placeholder":"​","style":"IPY_MODEL_3f268e5af2b946a2b1821dd7c7d4dcda","value":" 386/386 [00:00&lt;00:00, 6.78kB/s]"}},"b4998147ee8b4f25a0798f95aa4dad38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b3478080ab4a7894012d1037e8c825":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6efde522637d444f895a5794a42e8c17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca1f8a7c77b44d0fa5465d103d7b9425":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3113a9ff1946168ac7ca3bb266e769":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db4d2fc9ae9e4fb894f97295ec061e5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f268e5af2b946a2b1821dd7c7d4dcda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ac120a85e704788a3cf991a83dacbca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5cdb3b064e4a4146a2efddf95ebee9d8","IPY_MODEL_15de174a0c084235b2b84716c98ecc04","IPY_MODEL_ff27ccc82b2b46f48d829127fabba96d"],"layout":"IPY_MODEL_6fd8f423b31f4813b55da4d62ac90932"}},"5cdb3b064e4a4146a2efddf95ebee9d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93724f3edb4e4b35b0253ca1f4b4073f","placeholder":"​","style":"IPY_MODEL_2fdda6a2dc5642f182dca7cd6a27e24d","value":"Downloading pytorch_model.bin: 100%"}},"15de174a0c084235b2b84716c98ecc04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8bd2049f5124d9ca44133d30261037d","max":740314769,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79f4ae14d892477386f578594a6a3e2e","value":740314769}},"ff27ccc82b2b46f48d829127fabba96d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_010811fbeb3d4ea0ae8214bec4164f85","placeholder":"​","style":"IPY_MODEL_48213bd29f8345c19e0275867d7ec072","value":" 740M/740M [00:04&lt;00:00, 140MB/s]"}},"6fd8f423b31f4813b55da4d62ac90932":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93724f3edb4e4b35b0253ca1f4b4073f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fdda6a2dc5642f182dca7cd6a27e24d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8bd2049f5124d9ca44133d30261037d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79f4ae14d892477386f578594a6a3e2e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"010811fbeb3d4ea0ae8214bec4164f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48213bd29f8345c19e0275867d7ec072":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install simpletransformers -q\n","metadata":{"id":"UxKYwtDwAbVB","execution":{"iopub.status.busy":"2023-04-04T19:06:36.638257Z","iopub.execute_input":"2023-04-04T19:06:36.638669Z","iopub.status.idle":"2023-04-04T19:06:58.855906Z","shell.execute_reply.started":"2023-04-04T19:06:36.638627Z","shell.execute_reply":"2023-04-04T19:06:58.854695Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport numpy as np \nimport shutil\nimport sys\nimport requests\n","metadata":{"id":"mvDkPLXBAT6B","execution":{"iopub.status.busy":"2023-04-04T19:06:58.859615Z","iopub.execute_input":"2023-04-04T19:06:58.860025Z","iopub.status.idle":"2023-04-04T19:07:01.580866Z","shell.execute_reply.started":"2023-04-04T19:06:58.859987Z","shell.execute_reply":"2023-04-04T19:07:01.579817Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/tf-train/teknofest_train_final (1).csv',delimiter='|')","metadata":{"id":"_YNZqqzbQRAx","execution":{"iopub.status.busy":"2023-04-04T19:07:01.582118Z","iopub.execute_input":"2023-04-04T19:07:01.582779Z","iopub.status.idle":"2023-04-04T19:07:01.666638Z","shell.execute_reply.started":"2023-04-04T19:07:01.582738Z","shell.execute_reply":"2023-04-04T19:07:01.665622Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"url = \"https://cryptic-oasis-68424.herokuapp.com/preprocess?tr_chars=false&acc_marks=true&punct=true&lower=true&offensive=false&norm_numbers=true&remove_numbers=false&remove_spaces=true&remove_stopwords=false&min_len=4\"\ntexts = df.text.values.tolist()\npreprocess_response = requests.post(url, json={\"texts\": texts})\nprocessed_text = preprocess_response.json()['result']\ndf.text = processed_text\nprint(df[df.text == ''].sum())\ndf = df[df['text'] != '']\nprint(df.head())\nprint(df[df.text == ''].sum())","metadata":{"execution":{"iopub.status.busy":"2023-04-04T19:07:01.669503Z","iopub.execute_input":"2023-04-04T19:07:01.669951Z","iopub.status.idle":"2023-04-04T19:07:07.050134Z","shell.execute_reply.started":"2023-04-04T19:07:01.669911Z","shell.execute_reply":"2023-04-04T19:07:07.049049Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"id              e2d954b7-266d-43be-845e-015a8ecf1241697c1629-d...\ntext                                                             \nis_offensive                                                   79\ntarget          RACISTRACISTRACISTRACISTSEXISTOTHEROTHEROTHERR...\ndtype: object\n                                     id  \\\n0  81c11060-a240-4d54-841b-9e2916039e85   \n1  be80ebbf-b322-4c3b-afa1-94932ea80731   \n2  f99e2513-83ed-4076-ac72-b9e2cff3f049   \n3  83ed2b2e-b815-4f36-9fc4-80a9050cf2d0   \n4  d93e05f7-bfdd-4cdb-99d8-3048761b30ff   \n\n                                                text  is_offensive     target  \n0                                        çürük dişli             1     INSULT  \n1  bu adamın islama ve müslümanlara verdiği zarar...             1     RACIST  \n2                               erkekler zora gelmez             1     SEXIST  \n3  utanmazın götüne kazık sokmuşlar bu tıkırtı ne...             1  PROFANITY  \n4       otomasyon sistemlerine doğrudan bağlanabilir             0      OTHER  \nid              0.0\ntext            0.0\nis_offensive    0.0\ntarget          0.0\ndtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"df.info()","metadata":{"id":"nUATCBh5CK4G","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b42fc096-b9a2-4767-9a85-123e10fa9c2d","execution":{"iopub.status.busy":"2023-04-04T19:07:07.051766Z","iopub.execute_input":"2023-04-04T19:07:07.052521Z","iopub.status.idle":"2023-04-04T19:07:07.072680Z","shell.execute_reply.started":"2023-04-04T19:07:07.052458Z","shell.execute_reply":"2023-04-04T19:07:07.071186Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12467 entries, 0 to 12616\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   id            12467 non-null  object\n 1   text          12467 non-null  object\n 2   is_offensive  12467 non-null  int64 \n 3   target        12467 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 487.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df_dmy=pd.get_dummies(df.target)","metadata":{"id":"Tj9LCaLhCK68","execution":{"iopub.status.busy":"2023-04-04T19:07:07.074224Z","iopub.execute_input":"2023-04-04T19:07:07.075329Z","iopub.status.idle":"2023-04-04T19:07:07.085284Z","shell.execute_reply.started":"2023-04-04T19:07:07.075274Z","shell.execute_reply":"2023-04-04T19:07:07.083610Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df=pd.concat([df.text, df_dmy], axis=1)","metadata":{"id":"-Bg65ktKCK9S","execution":{"iopub.status.busy":"2023-04-04T19:07:07.087301Z","iopub.execute_input":"2023-04-04T19:07:07.088074Z","iopub.status.idle":"2023-04-04T19:07:07.095834Z","shell.execute_reply.started":"2023-04-04T19:07:07.088034Z","shell.execute_reply":"2023-04-04T19:07:07.094450Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"4y7lP1AqCK_s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d3bd8615-be65-4c2b-b010-d3623f91edbe","execution":{"iopub.status.busy":"2023-04-04T19:07:07.097846Z","iopub.execute_input":"2023-04-04T19:07:07.098808Z","iopub.status.idle":"2023-04-04T19:07:07.113597Z","shell.execute_reply.started":"2023-04-04T19:07:07.098766Z","shell.execute_reply":"2023-04-04T19:07:07.112365Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 12467 entries, 0 to 12616\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   text       12467 non-null  object\n 1   INSULT     12467 non-null  uint8 \n 2   OTHER      12467 non-null  uint8 \n 3   PROFANITY  12467 non-null  uint8 \n 4   RACIST     12467 non-null  uint8 \n 5   SEXIST     12467 non-null  uint8 \ndtypes: object(1), uint8(5)\nmemory usage: 255.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"target_list=['INSULT','OTHER','PROFANITY','RACIST','SEXIST']","metadata":{"id":"u75N6220CLCS","execution":{"iopub.status.busy":"2023-04-04T19:07:07.115588Z","iopub.execute_input":"2023-04-04T19:07:07.116263Z","iopub.status.idle":"2023-04-04T19:07:07.121408Z","shell.execute_reply.started":"2023-04-04T19:07:07.116219Z","shell.execute_reply":"2023-04-04T19:07:07.120067Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df=df.sample(frac=0.70,random_state=52025).reset_index(drop=True)\nval_df=df.drop(train_df.index).reset_index(drop=True)\n\ntest_df=val_df.sample(frac=0.25,random_state=52025).reset_index(drop=True)\nval_df=val_df.drop(test_df.index).reset_index(drop=True)","metadata":{"id":"sHOLy_p9CLMR","execution":{"iopub.status.busy":"2023-04-04T19:07:07.127409Z","iopub.execute_input":"2023-04-04T19:07:07.128008Z","iopub.status.idle":"2023-04-04T19:07:07.144381Z","shell.execute_reply.started":"2023-04-04T19:07:07.127976Z","shell.execute_reply":"2023-04-04T19:07:07.143359Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"val_df.info()","metadata":{"id":"LrTwz4Agdvty","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8cb90221-4412-45f6-8e4a-1184b5f114b6","execution":{"iopub.status.busy":"2023-04-04T19:07:07.145777Z","iopub.execute_input":"2023-04-04T19:07:07.146372Z","iopub.status.idle":"2023-04-04T19:07:07.158794Z","shell.execute_reply.started":"2023-04-04T19:07:07.146330Z","shell.execute_reply":"2023-04-04T19:07:07.157465Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2805 entries, 0 to 2804\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   text       2805 non-null   object\n 1   INSULT     2805 non-null   uint8 \n 2   OTHER      2805 non-null   uint8 \n 3   PROFANITY  2805 non-null   uint8 \n 4   RACIST     2805 non-null   uint8 \n 5   SEXIST     2805 non-null   uint8 \ndtypes: object(1), uint8(5)\nmemory usage: 35.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.info()","metadata":{"id":"KcQVKJc_CLOz","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f163ff3-f5cc-454d-d84c-5fc7a96bb8a0","execution":{"iopub.status.busy":"2023-04-04T19:07:07.160597Z","iopub.execute_input":"2023-04-04T19:07:07.161245Z","iopub.status.idle":"2023-04-04T19:07:07.177620Z","shell.execute_reply.started":"2023-04-04T19:07:07.161204Z","shell.execute_reply":"2023-04-04T19:07:07.176673Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8727 entries, 0 to 8726\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   text       8727 non-null   object\n 1   INSULT     8727 non-null   uint8 \n 2   OTHER      8727 non-null   uint8 \n 3   PROFANITY  8727 non-null   uint8 \n 4   RACIST     8727 non-null   uint8 \n 5   SEXIST     8727 non-null   uint8 \ndtypes: object(1), uint8(5)\nmemory usage: 110.9+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df.info()","metadata":{"id":"0_VwowWNwYVy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9843678b-53b5-44af-aa62-bb81e47141ba","execution":{"iopub.status.busy":"2023-04-04T19:07:07.178922Z","iopub.execute_input":"2023-04-04T19:07:07.179479Z","iopub.status.idle":"2023-04-04T19:07:07.193359Z","shell.execute_reply.started":"2023-04-04T19:07:07.179437Z","shell.execute_reply":"2023-04-04T19:07:07.192156Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 935 entries, 0 to 934\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   text       935 non-null    object\n 1   INSULT     935 non-null    uint8 \n 2   OTHER      935 non-null    uint8 \n 3   PROFANITY  935 non-null    uint8 \n 4   RACIST     935 non-null    uint8 \n 5   SEXIST     935 non-null    uint8 \ndtypes: object(1), uint8(5)\nmemory usage: 12.0+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df[target_list].values","metadata":{"id":"JYUOJQeF_nz4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2d30e2fa-b944-48f5-c224-d11413de6d85","execution":{"iopub.status.busy":"2023-04-04T19:07:07.195362Z","iopub.execute_input":"2023-04-04T19:07:07.195766Z","iopub.status.idle":"2023-04-04T19:07:07.207296Z","shell.execute_reply.started":"2023-04-04T19:07:07.195727Z","shell.execute_reply":"2023-04-04T19:07:07.206154Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([[0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0],\n       ...,\n       [1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0]], dtype=uint8)"},"metadata":{}}]},{"cell_type":"code","source":"#bazı hyperparametreler..\nMAX_LEN=40\nTRAIN_BATCH_SIZE=32\nVALID_BATCH_SIZE=32\nEPOCHS=7\nLR=5e-5\nLM = \"dbmdz/bert-base-turkish-128k-uncased\"","metadata":{"id":"aqOWiDKTCLEc","execution":{"iopub.status.busy":"2023-04-04T19:07:07.209144Z","iopub.execute_input":"2023-04-04T19:07:07.209599Z","iopub.status.idle":"2023-04-04T19:07:07.215239Z","shell.execute_reply.started":"2023-04-04T19:07:07.209558Z","shell.execute_reply":"2023-04-04T19:07:07.213875Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel","metadata":{"id":"fujATdOrCLHM","execution":{"iopub.status.busy":"2023-04-04T19:07:07.217249Z","iopub.execute_input":"2023-04-04T19:07:07.217925Z","iopub.status.idle":"2023-04-04T19:07:10.331135Z","shell.execute_reply.started":"2023-04-04T19:07:07.217871Z","shell.execute_reply":"2023-04-04T19:07:10.329927Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(LM, do_lower_case=True, use_cuda=torch.cuda.is_available())\ntokenizer","metadata":{"id":"zkxxhFQCCLJi","colab":{"base_uri":"https://localhost:8080/","height":149,"referenced_widgets":["0ed04673c64748b0b44d44bf56e9d804","27286bba47f745d7938122553f983ded","63915f7302fc461e99b7ee6ff6c7e294","2f50e751226a478f8ed5645af1c3062b","b7e9499133b34ff5986b1b4ce8bb3d73","1c16eafff2184d1c9ecde97629b3bd6c","98d1b0ef1e294f639c035bbd1faa6e14","3ced53e036b8430eada4d3132d8a6641","a54216b98e094e78b0166cece05d15cb","a3d7f6284ec042e9a77aee68002e8692","441d5f99cdbb4c9aa7321e6a5e09024f","dee1d1835fcd42d69f0b343009a5453d","980d61c0dd5245dc9ce34ba0be168dd7","b482ee544ecc464780f3534f6150ca65","c03dd6d5b99940cf87aae6bd82a07da4","de52ce89909343c6948ad29a4e3793f8","17e37e17d8314833ae4c30cd66695acd","7169b1d67d8545bebc2c4fc533a7e30a","f8f5f8c3ff854a8f9093d745c489972d","a6573d3f8f304e2089ef231aef462330","c9c350e5dbc14651b9fc4bcb840f5082","7ee5c5c006b24a9db6f247af5148f12b","724209c6fd314416811813dc7b6c360c","ff61b71a09974846ae20c71a881f7d79","b193579ca9de44c089e9011b464e754b","fe8e89f3fcfa4caba30248973b371279","b4998147ee8b4f25a0798f95aa4dad38","b4b3478080ab4a7894012d1037e8c825","6efde522637d444f895a5794a42e8c17","ca1f8a7c77b44d0fa5465d103d7b9425","6d3113a9ff1946168ac7ca3bb266e769","db4d2fc9ae9e4fb894f97295ec061e5b","3f268e5af2b946a2b1821dd7c7d4dcda"]},"outputId":"324cc538-6037-4e9b-df8e-cffb3df38403","execution":{"iopub.status.busy":"2023-04-04T19:07:10.333445Z","iopub.execute_input":"2023-04-04T19:07:10.333846Z","iopub.status.idle":"2023-04-04T19:07:14.116943Z","shell.execute_reply.started":"2023-04-04T19:07:10.333801Z","shell.execute_reply":"2023-04-04T19:07:14.115710Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e102e9d9a2304294a5d22a8eac5873d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ae891a1ed14d8f96d0668fcbf29c55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/386 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45674f24f79487ab85f31a63ece4bd3"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"BertTokenizer(name_or_path='dbmdz/bert-base-turkish-128k-uncased', vocab_size=128000, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"metadata":{}}]},{"cell_type":"code","source":"exm=\" elif ben sana ne demiştim kızım, çocuğu uyuturken uyuya kalma demedim mi # ? \"\nencodings=tokenizer.encode_plus(exm,\n                                add_special_tokens=True,\n                                max_length=MAX_LEN,\n                                padding='max_length',\n                                truncation=True,\n                                return_attention_mask=True,\n                                return_tensors='pt')","metadata":{"id":"oZaw1hfB1Ftj","execution":{"iopub.status.busy":"2023-04-04T19:07:14.118536Z","iopub.execute_input":"2023-04-04T19:07:14.119122Z","iopub.status.idle":"2023-04-04T19:07:14.138797Z","shell.execute_reply.started":"2023-04-04T19:07:14.119082Z","shell.execute_reply":"2023-04-04T19:07:14.137660Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"encodings","metadata":{"id":"dfio0qpA1FwR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a5031c6-d718-410a-f3b7-73e712e829bf","execution":{"iopub.status.busy":"2023-04-04T19:07:14.140328Z","iopub.execute_input":"2023-04-04T19:07:14.141180Z","iopub.status.idle":"2023-04-04T19:07:14.167176Z","shell.execute_reply.started":"2023-04-04T19:07:14.141134Z","shell.execute_reply":"2023-04-04T19:07:14.166003Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[    2, 12626,  2105,  3247,  2073, 71015,  3180,  9946,    16, 89973,\n         27716,  5551, 21660,  6034, 30094,  2477,     7,    35,     3,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}]},{"cell_type":"code","source":"train_df['text'][0]\nprint(train_df.values)","metadata":{"id":"91du3VxF7qKi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5dce754c-fcb0-43fd-c661-08d136b97229","execution":{"iopub.status.busy":"2023-04-04T19:07:14.168904Z","iopub.execute_input":"2023-04-04T19:07:14.169277Z","iopub.status.idle":"2023-04-04T19:07:14.178758Z","shell.execute_reply.started":"2023-04-04T19:07:14.169239Z","shell.execute_reply":"2023-04-04T19:07:14.177585Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[['ben canım sevgilimi ararım' 0 1 0 0 0]\n ['sendeki sinsilik yılanda yok' 1 0 0 0 0]\n ['sikik beyinsiz' 0 0 1 0 0]\n ...\n ['senin kadar çirkin bir insan görmediğime yemin edebilirim ama ıspatlayamam'\n  1 0 0 0 0]\n ['oldukça memnun kaldığımı ve çoğu kişiye özellikle fiyat performansı açısından önerdiğimi belirteyim'\n  0 1 0 0 0]\n ['seni isteyen kimse kalmadı' 1 0 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"target_list","metadata":{"id":"Sg32OqVw-jye","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c695033-ae74-4464-d230-052c6cbe5613","execution":{"iopub.status.busy":"2023-04-04T19:07:14.180618Z","iopub.execute_input":"2023-04-04T19:07:14.181442Z","iopub.status.idle":"2023-04-04T19:07:14.188696Z","shell.execute_reply.started":"2023-04-04T19:07:14.181403Z","shell.execute_reply":"2023-04-04T19:07:14.187436Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['INSULT', 'OTHER', 'PROFANITY', 'RACIST', 'SEXIST']"},"metadata":{}}]},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.df = df\n        self.title = df['text']\n        self.targets = self.df[target_list].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.title)\n\n    def __getitem__(self, index):\n        title = str(self.title[index])\n        title = \" \".join(title.split())\n\n        inputs = self.tokenizer.encode_plus(\n            title,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n            'targets': torch.FloatTensor(self.targets[index])\n        }","metadata":{"id":"VKf26-3HL7o1","execution":{"iopub.status.busy":"2023-04-04T19:07:14.190508Z","iopub.execute_input":"2023-04-04T19:07:14.191282Z","iopub.status.idle":"2023-04-04T19:07:14.200362Z","shell.execute_reply.started":"2023-04-04T19:07:14.191221Z","shell.execute_reply":"2023-04-04T19:07:14.199340Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\nvalid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)","metadata":{"id":"Tj0G9Gur7p6s","execution":{"iopub.status.busy":"2023-04-04T19:07:14.202093Z","iopub.execute_input":"2023-04-04T19:07:14.202552Z","iopub.status.idle":"2023-04-04T19:07:14.211672Z","shell.execute_reply.started":"2023-04-04T19:07:14.202513Z","shell.execute_reply":"2023-04-04T19:07:14.210540Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(train_dataset, \n    batch_size=TRAIN_BATCH_SIZE,\n    shuffle=True,\n    num_workers=0\n)\n\nval_data_loader = torch.utils.data.DataLoader(valid_dataset, \n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"id":"kssvO5CAADb8","execution":{"iopub.status.busy":"2023-04-04T19:07:14.213346Z","iopub.execute_input":"2023-04-04T19:07:14.213830Z","iopub.status.idle":"2023-04-04T19:07:14.220842Z","shell.execute_reply.started":"2023-04-04T19:07:14.213792Z","shell.execute_reply":"2023-04-04T19:07:14.219637Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(len(train_data_loader))\nprint(len(val_data_loader))","metadata":{"id":"M0reTMrkOKk2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"688883a0-353a-4eb8-a85b-d12fa631aa56","execution":{"iopub.status.busy":"2023-04-04T19:07:14.223601Z","iopub.execute_input":"2023-04-04T19:07:14.223982Z","iopub.status.idle":"2023-04-04T19:07:14.232889Z","shell.execute_reply.started":"2023-04-04T19:07:14.223946Z","shell.execute_reply":"2023-04-04T19:07:14.231729Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"273\n88\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"id":"0L8z4sW4ADZG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7a8f62c-ebbd-49ae-d200-1aef37f708e2","execution":{"iopub.status.busy":"2023-04-04T19:07:14.234387Z","iopub.execute_input":"2023-04-04T19:07:14.235295Z","iopub.status.idle":"2023-04-04T19:07:14.245626Z","shell.execute_reply.started":"2023-04-04T19:07:14.235253Z","shell.execute_reply":"2023-04-04T19:07:14.244361Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"def load_ckp(checkpoint_fpath, model, optimizer):\n    \"\"\"\n    checkpoint_path: path to save checkpoint\n    model: model that we want to load checkpoint parameters into       \n    optimizer: optimizer we defined in previous training\n    \"\"\"\n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['state_dict'])\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    valid_loss_min = checkpoint['valid_loss_min']\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n","metadata":{"id":"QqpxSeMdADWe","execution":{"iopub.status.busy":"2023-04-04T19:07:14.251030Z","iopub.execute_input":"2023-04-04T19:07:14.251888Z","iopub.status.idle":"2023-04-04T19:07:14.265770Z","shell.execute_reply.started":"2023-04-04T19:07:14.251850Z","shell.execute_reply":"2023-04-04T19:07:14.264542Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def save_ckp(state, is_best, checkpoint_path, best_model_path):\n    \"\"\"\n    state: checkpoint we want to save\n    is_best: is this the best checkpoint; min validation loss\n    checkpoint_path: path to save checkpoint\n    best_model_path: path to save best model\n    \"\"\"\n    f_path = checkpoint_path\n    # save checkpoint data to the path given, checkpoint_path\n    torch.save(state, f_path)\n    # if it is a best model, min validation loss\n    if is_best:\n        best_fpath = best_model_path\n        # copy that checkpoint file to best path given, best_model_path\n        shutil.copyfile(f_path, best_fpath)","metadata":{"id":"x0Ne0A-BWD6W","execution":{"iopub.status.busy":"2023-04-04T19:07:14.273411Z","iopub.execute_input":"2023-04-04T19:07:14.274429Z","iopub.status.idle":"2023-04-04T19:07:14.281130Z","shell.execute_reply.started":"2023-04-04T19:07:14.274387Z","shell.execute_reply":"2023-04-04T19:07:14.279765Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class BERTClass(torch.nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert_model = BertModel.from_pretrained('dbmdz/bert-base-turkish-128k-uncased', return_dict=True)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768, 5)#çünkü 5 label var\n    \n    def forward(self, input_ids, attn_mask, token_type_ids):\n        output = self.bert_model(\n            input_ids, \n            attention_mask=attn_mask, \n            token_type_ids=token_type_ids\n        )\n        output_dropout = self.dropout(output.pooler_output)\n        output = self.linear(output_dropout)\n        return output\n\nmodel = BERTClass()\nmodel.to(device)","metadata":{"id":"ybIJDlvyADT9","colab":{"base_uri":"https://localhost:8080/","height":926,"referenced_widgets":["8ac120a85e704788a3cf991a83dacbca","5cdb3b064e4a4146a2efddf95ebee9d8","15de174a0c084235b2b84716c98ecc04","ff27ccc82b2b46f48d829127fabba96d","6fd8f423b31f4813b55da4d62ac90932","93724f3edb4e4b35b0253ca1f4b4073f","2fdda6a2dc5642f182dca7cd6a27e24d","d8bd2049f5124d9ca44133d30261037d","79f4ae14d892477386f578594a6a3e2e","010811fbeb3d4ea0ae8214bec4164f85","48213bd29f8345c19e0275867d7ec072"]},"outputId":"a374d59e-9eb3-44a3-86d2-68b488fcaafa","execution":{"iopub.status.busy":"2023-04-04T19:07:14.282754Z","iopub.execute_input":"2023-04-04T19:07:14.284944Z","iopub.status.idle":"2023-04-04T19:07:24.613217Z","shell.execute_reply.started":"2023-04-04T19:07:14.284905Z","shell.execute_reply":"2023-04-04T19:07:24.612040Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/740M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee39f7d387b491bb27936e0952d54f1"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"BERTClass(\n  (bert_model): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\noptimizer = torch.optim.AdamW(params =  model.parameters(), lr=LR, eps = 1e-7)","metadata":{"id":"P7qCqF9yADP7","execution":{"iopub.status.busy":"2023-04-04T19:07:24.614741Z","iopub.execute_input":"2023-04-04T19:07:24.615918Z","iopub.status.idle":"2023-04-04T19:07:24.622820Z","shell.execute_reply.started":"2023-04-04T19:07:24.615875Z","shell.execute_reply":"2023-04-04T19:07:24.621678Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"val_targets=[]\nval_outputs=[]\n     ","metadata":{"id":"oFbr0rdpKy4q","execution":{"iopub.status.busy":"2023-04-04T19:07:24.625508Z","iopub.execute_input":"2023-04-04T19:07:24.626150Z","iopub.status.idle":"2023-04-04T19:07:27.586619Z","shell.execute_reply.started":"2023-04-04T19:07:24.626108Z","shell.execute_reply":"2023-04-04T19:07:27.585313Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def train_model(n_epochs, training_loader, validation_loader, model, \n                optimizer, checkpoint_path, best_model_path):\n  print(len(training_loader))\n  print(len(validation_loader))\n  # initialize tracker for minimum validation loss\n  valid_loss_min = np.Inf\n   \n \n  for epoch in range(1, n_epochs+1):\n    train_loss = 0\n    valid_loss = 0\n\n    model.train()\n    print('############# Epoch {}: Training Start   #############'.format(epoch))\n    for batch_idx, data in enumerate(training_loader):\n        #print('yyy epoch', batch_idx)\n        ids = data['input_ids'].to(device, dtype = torch.long)\n        mask = data['attention_mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        #if batch_idx%5000==0:\n         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        #print('before loss data in training', loss.item(), train_loss)\n        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n        #print('after loss data in training', loss.item(), train_loss)\n    \n    print('############# Epoch {}: Training End     #############'.format(epoch))\n    \n    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n    ######################    \n    # validate the model #\n    ######################\n \n    model.eval()\n   \n    with torch.no_grad():\n      for batch_idx, data in enumerate(validation_loader, 0):\n            ids = data['input_ids'].to(device, dtype = torch.long)\n            mask = data['attention_mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n\n            loss = loss_fn(outputs, targets)\n            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n            val_targets.extend(targets.cpu().detach().numpy().tolist())\n            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n\n      print('############# Epoch {}: Validation End     #############'.format(epoch))\n      # calculate average losses\n      #print('before cal avg train loss', train_loss)\n      train_loss = train_loss/len(training_loader)\n      print(train_loss)\n      valid_loss = valid_loss/len(validation_loader)\n      print(valid_loss)\n      # print training/validation statistics \n      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n            epoch, \n            train_loss,\n            valid_loss\n            ))\n      \n      # create checkpoint variable and add important data\n      checkpoint = {\n            'epoch': epoch + 1,\n            'valid_loss_min': valid_loss,\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict()\n      }\n        \n        # save checkpoint\n      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n        \n      ## TODO: save the model if validation loss has decreased\n      if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n        # save checkpoint as best model\n        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n        valid_loss_min = valid_loss\n\n    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n\n  return model","metadata":{"id":"-cIpMuI1Ky7A","execution":{"iopub.status.busy":"2023-04-04T19:07:27.588449Z","iopub.execute_input":"2023-04-04T19:07:27.588896Z","iopub.status.idle":"2023-04-04T19:07:27.614231Z","shell.execute_reply.started":"2023-04-04T19:07:27.588857Z","shell.execute_reply":"2023-04-04T19:07:27.613134Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"ckpt_path = \"/kaggle/working/v1_curr_ckpt\"\nbest_model_path = \"/kaggle/working/v1_best_model.pt\"\n\n#tokenizer.save_pretrained(\"/content/drive/MyDrive/Nane&Limon/2023 DDI Yarışma dokümantasyonu/bigscience_t0_tokenizer\")\n#model.save_pretrained(\"/content/drive/MyDrive/Nane&Limon/2023 DDI Yarışma dokümantasyonu/bigscience_t0_model\")","metadata":{"id":"r3yUpqN6Ky9n","execution":{"iopub.status.busy":"2023-04-04T19:07:27.615685Z","iopub.execute_input":"2023-04-04T19:07:27.616623Z","iopub.status.idle":"2023-04-04T19:07:27.625413Z","shell.execute_reply.started":"2023-04-04T19:07:27.616579Z","shell.execute_reply":"2023-04-04T19:07:27.624248Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vilL3q-tKy_3","outputId":"159b815c-dd12-49a3-bc01-96d96b80d5fe","execution":{"iopub.status.busy":"2023-04-04T19:07:27.627123Z","iopub.execute_input":"2023-04-04T19:07:27.627860Z","iopub.status.idle":"2023-04-04T19:15:36.380357Z","shell.execute_reply.started":"2023-04-04T19:07:27.627820Z","shell.execute_reply":"2023-04-04T19:15:36.379183Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"273\n88\n############# Epoch 1: Training Start   #############\n############# Epoch 1: Training End     #############\n############# Epoch 1: Validation Start   #############\n############# Epoch 1: Validation End     #############\n0.000664835443394848\n0.0008608391433896511\nEpoch: 1 \tAvgerage Training Loss: 0.000665 \tAverage Validation Loss: 0.000861\nValidation loss decreased (inf --> 0.000861).  Saving model ...\n############# Epoch 1  Done   #############\n\n############# Epoch 2: Training Start   #############\n############# Epoch 2: Training End     #############\n############# Epoch 2: Validation Start   #############\n############# Epoch 2: Validation End     #############\n0.00022600194156305043\n0.0006255817367245006\nEpoch: 2 \tAvgerage Training Loss: 0.000226 \tAverage Validation Loss: 0.000626\nValidation loss decreased (0.000861 --> 0.000626).  Saving model ...\n############# Epoch 2  Done   #############\n\n############# Epoch 3: Training Start   #############\n############# Epoch 3: Training End     #############\n############# Epoch 3: Validation Start   #############\n############# Epoch 3: Validation End     #############\n0.0001414619900361075\n0.00047446775619587145\nEpoch: 3 \tAvgerage Training Loss: 0.000141 \tAverage Validation Loss: 0.000474\nValidation loss decreased (0.000626 --> 0.000474).  Saving model ...\n############# Epoch 3  Done   #############\n\n############# Epoch 4: Training Start   #############\n############# Epoch 4: Training End     #############\n############# Epoch 4: Validation Start   #############\n############# Epoch 4: Validation End     #############\n8.521115005447787e-05\n0.0006120225619923994\nEpoch: 4 \tAvgerage Training Loss: 0.000085 \tAverage Validation Loss: 0.000612\n############# Epoch 4  Done   #############\n\n############# Epoch 5: Training Start   #############\n############# Epoch 5: Training End     #############\n############# Epoch 5: Validation Start   #############\n############# Epoch 5: Validation End     #############\n8.732212262313875e-05\n0.0006006616038440587\nEpoch: 5 \tAvgerage Training Loss: 0.000087 \tAverage Validation Loss: 0.000601\n############# Epoch 5  Done   #############\n\n############# Epoch 6: Training Start   #############\n############# Epoch 6: Training End     #############\n############# Epoch 6: Validation Start   #############\n############# Epoch 6: Validation End     #############\n6.275949922838979e-05\n0.000525574726189683\nEpoch: 6 \tAvgerage Training Loss: 0.000063 \tAverage Validation Loss: 0.000526\n############# Epoch 6  Done   #############\n\n############# Epoch 7: Training Start   #############\n############# Epoch 7: Training End     #############\n############# Epoch 7: Validation Start   #############\n############# Epoch 7: Validation End     #############\n5.6074534660109594e-05\n0.0005423897858213609\nEpoch: 7 \tAvgerage Training Loss: 0.000056 \tAverage Validation Loss: 0.000542\n############# Epoch 7  Done   #############\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ntest_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, \n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n\n# modeli test verisi ile değerlendirme\nmodel.eval()\ny_true = []\ny_pred = []\nwith torch.no_grad():\n  for data in test_data_loader:\n    input_ids = data['input_ids'].to(device)\n    attention_mask = data['attention_mask'].to(device)\n    token_type_ids = data[\"token_type_ids\"].to(device)\n    targets = data['targets'].to(device)\n    outputs = model(input_ids, attention_mask, token_type_ids)\n    final_output = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n    y_true.extend(targets.cpu().numpy())\n    y_pred.extend(np.round(final_output))\n      \n# F1 score macro\nf1_macro = f1_score(y_true, y_pred, average='macro')\nprint(\"F1 score (macro) : {:.5f}%\".format(f1_macro*100))\n","metadata":{"id":"2uwg9F0Lo9JU","execution":{"iopub.status.busy":"2023-04-04T19:15:36.382313Z","iopub.execute_input":"2023-04-04T19:15:36.382718Z","iopub.status.idle":"2023-04-04T19:15:39.287540Z","shell.execute_reply.started":"2023-04-04T19:15:36.382680Z","shell.execute_reply":"2023-04-04T19:15:39.286340Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"F1 score (macro) : 97.55665%\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer = BertTokenizer.from_pretrained(LM)\nmodel = BertModel.from_pretrained(LM)\n\n# Modelinizi ve tokenizer'ı yükleyin\n\ntorch.load(best_model_path, map_location=torch.device('cpu'))\nprint(type(model)) # dict dönüyor\n\n# Örnek bir girdi oluşturun\nexm = \"elif ben sana ne demiştim kızım, çocuğu uyuturken uyuya kalma demedim mi # ?\"\nexample = 'sıska bacaklı göt adam'\n\nMAX_LEN = 128  # örnek bir maksimum girdi uzunluğu\n# Örnek girdiyi modeli kullanarak tahmin edin\nmodel.eval()\n\n#girdi tensoru oluştur\nencodings = tokenizer.encode_plus(\n    exm,\n    None,\n    add_special_tokens=True,\n    max_length=MAX_LEN,\n    padding='max_length',\n    return_token_type_ids=True,\n    truncation=True,\n    return_attention_mask=True,\n    return_tensors='pt'\n)\n\n# tensörleri atayın\ninput_ids = encodings['input_ids'].to('cpu', dtype=torch.long)\nattention_mask = encodings['attention_mask'].to('cpu', dtype=torch.long)\ntoken_type_ids = encodings['token_type_ids'].to('cpu', dtype=torch.long)\n\n# Tahmin yapın\noutput = model(input_ids, attention_mask, token_type_ids)\n\n# Tahminleri ekrana yazdırın\nprint(output)\n","metadata":{"id":"5j-zY_yRceCi","execution":{"iopub.status.busy":"2023-04-04T19:15:39.289410Z","iopub.execute_input":"2023-04-04T19:15:39.289817Z","iopub.status.idle":"2023-04-04T19:15:57.176728Z","shell.execute_reply.started":"2023-04-04T19:15:39.289777Z","shell.execute_reply":"2023-04-04T19:15:57.175559Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"<class 'transformers.models.bert.modeling_bert.BertModel'>\nBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-1.8387, -1.1023, -0.8242,  ...,  0.1805,  0.6291,  0.1831],\n         [-0.9457, -0.0307, -0.0440,  ...,  0.3992,  2.1232,  1.1942],\n         [-0.5214, -0.4406, -0.3887,  ..., -0.3835,  1.1899,  1.3980],\n         ...,\n         [-1.4262, -1.2720, -1.4024,  ...,  0.2835,  1.2293,  0.9535],\n         [-1.8355, -1.6825, -0.8395,  ...,  0.4935,  1.6269,  0.0179],\n         [-2.2859, -2.2545, -0.4670,  ...,  0.1839,  1.8497, -0.3740]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 1.0000e+00,  1.4090e-01,  9.9942e-01,  1.2597e-01,  1.7111e-02,\n         -1.0000e+00, -3.1108e-01, -9.9998e-01, -9.9549e-01,  9.9999e-01,\n         -1.1769e-01,  6.1526e-02,  3.7583e-01, -1.0000e+00, -9.9795e-01,\n         -6.1912e-01, -2.1775e-01,  1.1792e-01,  8.2704e-01,  9.9979e-01,\n         -1.6528e-01,  1.2525e-01,  9.9211e-01, -1.4756e-01,  3.1955e-01,\n          1.6565e-01,  9.6855e-01, -3.9256e-01, -5.1710e-01, -5.0473e-01,\n         -9.9994e-01,  9.9990e-01,  6.4651e-02,  1.8967e-01, -9.9935e-01,\n         -9.9997e-01,  9.9711e-01, -1.2905e-01, -2.2615e-01, -2.2817e-02,\n         -3.3938e-01,  1.0521e-01, -1.7129e-01, -9.6343e-01, -1.8013e-01,\n          9.9858e-01, -9.8909e-01, -1.5108e-01, -7.2055e-03,  8.8164e-01,\n          9.9997e-01, -9.9158e-01, -9.9999e-01,  9.6611e-01, -1.9715e-01,\n         -5.7012e-02, -3.1198e-02,  1.0000e+00, -3.8771e-01, -7.2243e-02,\n         -1.0000e+00, -7.2906e-03, -1.5033e-01,  1.0000e+00,  5.4582e-01,\n          1.3384e-01, -9.7905e-01, -1.7251e-01, -9.0251e-02,  2.1773e-01,\n          3.7999e-01, -4.8100e-01,  1.3442e-01, -6.5421e-02, -9.9982e-01,\n         -4.7931e-03,  9.9992e-01,  9.1024e-01, -9.9980e-01, -9.9310e-01,\n         -7.0676e-01, -2.2041e-01, -1.0000e+00, -3.7968e-01, -9.9247e-01,\n          2.2616e-01, -9.8998e-01, -8.8244e-02, -1.6730e-01,  9.9996e-01,\n          5.6855e-01, -9.9999e-01,  3.2472e-01,  9.7110e-01,  9.9989e-01,\n         -7.7887e-01, -2.7403e-01,  9.9777e-01,  9.9705e-01, -9.9997e-01,\n          9.9999e-01,  9.9994e-01, -1.6444e-01, -1.9062e-01,  2.7642e-01,\n         -9.9808e-01, -2.1983e-01,  1.0000e+00,  9.9998e-01, -9.9157e-01,\n         -1.0000e+00, -9.9910e-01, -9.9999e-01, -2.0941e-01, -3.6489e-03,\n         -2.8788e-01, -9.9668e-01, -1.0000e+00,  9.9986e-01, -9.9972e-01,\n         -1.3279e-01,  9.1796e-02, -8.1375e-01,  6.2161e-02,  2.4476e-01,\n          2.6339e-01,  3.6115e-01,  3.9776e-02, -5.5502e-01, -9.3058e-01,\n          5.0937e-01,  6.6748e-04, -9.9958e-01,  1.0000e+00, -1.4340e-01,\n         -9.9999e-01, -9.9985e-01, -1.9720e-01,  1.0000e+00,  1.4402e-01,\n         -4.2440e-01,  9.9999e-01,  9.8301e-01, -1.8691e-01,  1.1171e-01,\n          1.5998e-02, -2.3506e-01,  1.8747e-01, -6.0706e-02, -9.9918e-01,\n          9.9507e-01, -9.1259e-01, -4.7289e-01,  1.1659e-02,  2.1993e-01,\n          1.8781e-01, -3.4119e-02, -5.6575e-02,  9.9895e-01,  2.0539e-02,\n          1.8119e-01,  4.5705e-02, -1.0000e+00,  9.8210e-01, -7.3717e-02,\n          4.2729e-01,  9.0379e-01,  3.5237e-01,  9.9997e-01,  9.9994e-01,\n         -9.9999e-01, -9.9996e-01, -9.7970e-01,  9.9983e-01,  1.0000e+00,\n         -9.8348e-01, -9.6355e-02,  4.8200e-01,  3.8373e-01, -9.7319e-02,\n          7.1387e-01, -9.9993e-01,  8.9130e-01, -9.1885e-01,  3.3792e-01,\n          9.1359e-02, -1.0000e+00, -9.6794e-01, -6.5642e-02,  5.0177e-01,\n         -9.9945e-01, -9.9966e-01,  1.8350e-01, -2.2436e-01,  9.9981e-01,\n          1.9630e-01, -9.9952e-01, -2.8892e-01, -8.7293e-01, -1.0000e+00,\n          4.5484e-01, -9.0192e-02,  8.3516e-01, -1.0000e+00, -9.9988e-01,\n          3.9216e-02,  9.8171e-01, -3.0701e-01, -7.7414e-02, -1.5936e-01,\n         -9.9895e-01, -3.6934e-01, -3.1976e-02,  4.3929e-01,  6.9847e-01,\n         -9.9993e-01, -2.2397e-01,  9.9897e-01,  1.0000e+00,  1.6944e-01,\n         -9.9999e-01,  9.9995e-01,  3.7928e-01,  9.9998e-01,  1.7405e-01,\n         -9.9990e-01,  9.9802e-01,  9.8098e-01, -5.1072e-01, -1.0000e+00,\n         -3.9387e-01,  9.9988e-01, -9.9996e-01,  9.2622e-01, -3.3966e-01,\n         -9.9903e-01, -5.2130e-01, -1.6925e-01,  6.3958e-02, -9.9713e-01,\n          9.9929e-01,  9.9957e-01, -1.0000e+00,  8.5478e-02, -9.9346e-01,\n         -9.9983e-01, -9.9796e-01, -2.3133e-01,  4.8813e-01, -1.4124e-02,\n          9.9997e-01, -1.8335e-02,  9.6479e-01, -9.9856e-01,  1.9067e-01,\n          8.7574e-02,  1.5199e-01, -9.9099e-01,  1.7668e-01,  8.6696e-01,\n         -2.8122e-02, -9.9003e-01,  6.2096e-02, -1.0000e+00, -1.0000e+00,\n          2.2108e-01, -1.8345e-03,  3.3706e-01,  8.9992e-02,  2.7899e-01,\n         -9.9873e-01, -5.1608e-01,  9.9996e-01, -2.0626e-01,  2.9058e-01,\n          9.8895e-01,  3.2978e-02, -9.9965e-01, -7.4940e-03, -2.6552e-01,\n          7.1234e-01,  9.9983e-01, -4.2694e-01, -1.0000e+00, -3.7924e-01,\n          9.9997e-01, -1.9546e-01,  9.9993e-01,  7.8331e-01, -9.7561e-01,\n          1.7443e-01,  2.2823e-01,  9.9991e-01, -6.3456e-03,  9.9761e-01,\n         -1.8215e-01, -6.2132e-01,  3.1057e-01,  1.4032e-02, -6.7458e-01,\n          5.1890e-01, -9.9938e-01,  9.9997e-01, -9.9982e-01,  9.9913e-01,\n         -3.6910e-01, -9.9750e-01,  1.0000e+00,  9.9999e-01,  9.5754e-02,\n         -4.1576e-01, -2.6294e-01,  9.6541e-01,  4.7863e-01,  2.9112e-02,\n          2.6985e-01,  7.7891e-01, -8.7651e-02, -3.2336e-01,  3.4076e-01,\n          9.8422e-01, -3.7249e-01,  1.0026e-01, -1.5242e-02, -1.5934e-02,\n          9.9970e-01,  7.6386e-01,  9.9957e-01, -6.2633e-01, -9.9984e-01,\n          9.9984e-01, -1.3998e-01,  1.1666e-01,  9.9999e-01,  1.5180e-01,\n         -1.0000e+00, -6.2160e-01, -6.1616e-02, -3.4171e-01, -3.2222e-02,\n         -9.9718e-01,  9.2832e-02, -9.9995e-01,  9.9993e-01, -7.3600e-02,\n         -1.0000e+00, -9.7774e-01, -9.9999e-01,  1.0000e+00,  4.7896e-01,\n         -9.9930e-01,  3.8568e-01, -1.0401e-01,  9.9994e-01, -3.2528e-01,\n          9.9610e-01,  6.6343e-01,  9.8960e-01, -9.9998e-01,  6.0334e-02,\n         -7.2902e-01, -1.6341e-01, -1.0000e+00,  8.5879e-01,  1.2276e-01,\n         -2.8987e-02,  3.1373e-01,  7.4987e-01, -4.9726e-01, -1.8549e-01,\n          5.5920e-03,  3.7747e-01, -2.3734e-01,  3.4916e-01, -1.6132e-01,\n          4.9475e-02, -2.9835e-01, -1.8443e-01, -9.9511e-01, -9.9998e-01,\n         -6.4958e-01,  9.9993e-01, -3.2114e-01, -3.8053e-01,  5.4494e-01,\n         -1.0000e+00, -9.8475e-01, -8.7055e-01, -9.6483e-01,  9.9937e-01,\n          6.2836e-02, -7.5348e-01, -9.8682e-01, -6.6019e-01, -9.8905e-01,\n          1.5484e-01,  4.6376e-01,  7.9584e-01, -2.5752e-01, -9.3329e-01,\n         -4.6469e-01,  9.9999e-01, -1.0000e+00,  9.9970e-01, -9.0837e-02,\n         -2.9359e-01, -2.6435e-01,  9.9994e-01,  1.0000e+00,  9.9887e-01,\n          9.9994e-01, -9.9978e-01,  2.5137e-01, -3.1194e-01, -2.3877e-01,\n         -9.9949e-01,  2.3899e-01,  2.4817e-01,  1.7311e-01,  9.9982e-01,\n         -1.4372e-01, -9.5943e-01, -8.9694e-02,  5.1483e-01, -6.5312e-01,\n          3.0906e-01, -9.9995e-01,  1.0000e+00,  1.2360e-01, -9.9948e-01,\n          7.3393e-02, -9.9999e-01, -1.6317e-01, -7.2313e-02,  1.0657e-01,\n          2.9530e-01, -6.2731e-02, -9.9998e-01,  9.9968e-01,  3.2507e-01,\n          9.9998e-01, -4.9379e-01, -9.9999e-01,  1.0000e+00,  1.0000e+00,\n          9.9983e-01, -1.0000e+00, -9.9943e-01,  7.1504e-02,  7.2421e-01,\n          2.3814e-01, -4.5767e-01, -9.8747e-01,  4.9731e-02, -1.0000e+00,\n         -9.3761e-01, -1.4333e-01,  9.9984e-01, -2.2176e-02, -3.9979e-01,\n          3.4653e-01, -1.6766e-01, -2.9040e-01, -9.9999e-01, -6.0666e-02,\n         -1.3365e-01,  3.9942e-02,  9.9998e-01, -5.4682e-02, -9.8558e-01,\n          7.4460e-02, -9.5394e-01,  9.9994e-01, -3.1197e-01,  7.6785e-02,\n          9.9696e-01,  3.3431e-01,  4.3797e-01,  9.9998e-01, -3.4387e-01,\n          9.9992e-01,  9.9930e-01,  1.8239e-01, -8.8540e-01,  2.2786e-01,\n         -1.7978e-01,  9.9993e-01,  3.8685e-01, -9.9985e-01, -2.2983e-02,\n         -1.0000e+00, -2.5407e-01, -9.9538e-01,  2.0933e-01,  8.9451e-02,\n          9.9999e-01, -9.9998e-01, -4.5363e-02,  8.4747e-03,  2.2214e-01,\n          9.5435e-01,  9.9998e-01, -3.0483e-01,  4.2919e-01,  9.9999e-01,\n         -1.0000e+00,  9.9032e-01,  9.9585e-01, -2.4020e-02,  2.1951e-01,\n         -9.9848e-01,  7.7491e-01,  7.0805e-01, -9.9965e-01,  2.5707e-01,\n         -7.1314e-01, -1.0000e+00, -9.9992e-01,  9.9941e-01,  4.5540e-01,\n         -2.0936e-01, -1.4469e-01,  2.4996e-01, -2.8544e-02,  9.2442e-01,\n          2.7892e-01,  9.9996e-01,  9.9964e-01, -7.9680e-02,  7.3315e-02,\n         -2.8064e-01, -3.4717e-02, -1.0000e+00,  2.5818e-01, -9.9591e-01,\n          1.2816e-01,  3.5625e-01,  9.9464e-01, -9.9860e-01,  8.5685e-01,\n         -4.8248e-01,  9.9999e-01, -9.9997e-01,  1.0000e+00, -1.3571e-01,\n         -9.9999e-01,  3.3794e-01, -9.9997e-01,  2.2145e-01, -1.8451e-01,\n          9.8418e-01, -2.8603e-01,  2.1847e-01,  2.5446e-01, -3.6729e-01,\n         -4.4792e-01, -2.5149e-01,  9.9425e-01, -2.6133e-01, -3.1692e-01,\n         -9.2242e-01,  9.9926e-01, -4.2169e-01,  9.6276e-01, -9.9716e-01,\n          3.5914e-01,  9.9837e-01,  2.7832e-01, -3.3454e-01, -2.3976e-01,\n         -5.6152e-02,  1.1300e-01,  9.8520e-01, -9.8890e-01,  1.0000e+00,\n          9.9967e-01,  9.9948e-01,  9.9987e-01, -2.0244e-01,  8.5474e-01,\n          3.6257e-01,  1.8134e-01, -6.2638e-02, -9.9706e-01, -4.5980e-01,\n         -7.9841e-02, -7.3109e-02,  1.6187e-01, -9.9998e-01,  4.0263e-01,\n          9.6826e-01,  7.9537e-01,  9.6258e-01,  2.1798e-01,  6.8793e-01,\n          8.4462e-01, -1.3997e-01, -8.3033e-01,  1.3421e-01,  1.6420e-01,\n          3.2941e-01,  4.2686e-01,  9.1361e-02,  9.9801e-01,  9.9991e-01,\n         -1.0000e+00,  7.1770e-02, -9.9997e-01, -2.3604e-01, -6.6573e-01,\n         -9.8294e-02, -2.3863e-01, -6.9321e-01,  9.9996e-01,  1.0000e+00,\n         -2.0526e-01, -8.0024e-01, -1.0000e+00, -9.9998e-01,  9.9990e-01,\n         -1.2062e-01, -3.7860e-01, -7.6748e-02,  2.7406e-01, -9.9307e-01,\n         -1.3190e-01,  1.4086e-01,  1.0000e+00,  6.0452e-02, -1.6617e-01,\n          1.7250e-02,  9.0453e-01, -1.2825e-01,  9.9999e-01,  1.8712e-01,\n         -9.9992e-01, -9.1162e-01,  2.1114e-01, -8.7284e-01,  9.2711e-01,\n          6.4671e-02, -5.1434e-01, -9.9998e-01, -3.2944e-01, -4.4004e-01,\n         -2.1916e-01, -2.1377e-01, -1.0000e+00, -2.5814e-01, -9.9997e-01,\n         -9.9997e-01,  2.0792e-01,  1.0000e+00,  8.2197e-02, -9.9990e-01,\n         -1.0317e-01, -9.9999e-01,  9.9709e-01, -6.0096e-02,  1.8582e-01,\n          9.9338e-01, -4.1571e-01, -2.2297e-01, -2.5697e-01,  9.9950e-01,\n         -1.0000e+00,  1.3737e-01, -2.0331e-01,  9.9998e-01,  5.1699e-01,\n         -1.3626e-01, -9.9983e-01, -5.0468e-01,  1.4377e-02, -9.9887e-01,\n         -9.9946e-01,  9.9244e-01, -5.2617e-02, -2.3995e-01,  6.0193e-01,\n          9.9768e-01,  9.9367e-01,  2.0852e-01,  1.7877e-01,  3.7245e-01,\n         -6.2381e-01,  1.9685e-01,  1.1446e-01,  5.0979e-01, -1.7801e-01,\n         -9.7023e-01,  9.9983e-01,  4.4419e-01,  2.0865e-01,  1.0000e+00,\n         -9.9654e-01, -2.2483e-01, -6.4763e-02,  9.9658e-01, -9.3286e-01,\n          3.9545e-01,  4.4318e-01,  9.9974e-01,  4.9637e-01,  9.9972e-01,\n         -3.8771e-02,  8.3648e-01,  1.5896e-01,  9.6972e-01,  9.9688e-01,\n         -1.4673e-01, -9.9954e-01,  4.2701e-01,  9.9975e-01,  1.2462e-01,\n          9.9254e-01, -2.0018e-01,  1.0000e+00,  8.0535e-02,  9.9559e-01,\n         -3.5818e-01, -1.2587e-01,  2.4422e-01, -9.9195e-01, -4.7397e-01,\n          3.7238e-01, -2.1101e-01,  8.1969e-01,  9.1088e-01,  9.9922e-01,\n          4.7666e-01,  9.1933e-01,  2.1143e-01, -4.8294e-01,  9.9899e-01,\n         -4.8314e-01, -2.5344e-01,  4.6102e-03,  2.4781e-01,  9.9749e-01,\n         -7.4269e-02, -9.9960e-01, -4.7085e-01,  5.7799e-01, -2.7301e-01,\n          3.1568e-01, -1.0000e+00, -4.0076e-01, -1.9069e-01,  9.7168e-01,\n          3.4054e-01, -4.0966e-01, -1.0428e-01,  3.5351e-01,  7.4729e-02,\n          6.7499e-01,  9.9996e-01,  1.2965e-01, -2.9684e-01,  9.4175e-01,\n          1.0756e-01, -9.8852e-01,  9.7781e-01,  9.9957e-01, -9.6595e-01,\n         -9.9999e-01,  1.1013e-01,  6.9830e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_save = model.config.to_json_file('/kaggle/working/config.json') # model config save","metadata":{"execution":{"iopub.status.busy":"2023-04-04T19:15:57.178411Z","iopub.execute_input":"2023-04-04T19:15:57.179121Z","iopub.status.idle":"2023-04-04T19:15:57.186603Z","shell.execute_reply.started":"2023-04-04T19:15:57.179080Z","shell.execute_reply":"2023-04-04T19:15:57.185537Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained('bert-turkish-multi-label-tokenizer')\nmodel.save_pretrained('bert-turkish-multi-label-model')","metadata":{"execution":{"iopub.status.busy":"2023-04-04T19:27:30.054266Z","iopub.execute_input":"2023-04-04T19:27:30.055206Z","iopub.status.idle":"2023-04-04T19:27:31.009613Z","shell.execute_reply.started":"2023-04-04T19:27:30.055163Z","shell.execute_reply":"2023-04-04T19:27:31.008371Z"},"trusted":true},"execution_count":40,"outputs":[]}]}